"""
æ¯”è¾ƒæ¨¡å‹æ¨ç†ç»“æœå’ŒçœŸå®æ ‡æ³¨æ•°æ®
è®¡ç®—æ£€æµ‹ç²¾åº¦æŒ‡æ ‡: Precision, Recall, mAP50ç­‰
"""
import os
import numpy as np
from collections import defaultdict

def parse_detections(txt_file):
    """
    è§£ææ£€æµ‹ç»“æœæ–‡ä»¶ (MOTæ ¼å¼)
    æ ¼å¼: frame_id, track_id, x, y, w, h, conf, -1, -1, -1
    è¿”å›: {frame_id: [(x, y, w, h, conf), ...]}
    """
    detections = defaultdict(list)
    
    if not os.path.exists(txt_file):
        print(f"âš ï¸  File not found: {txt_file}")
        return detections
    
    with open(txt_file, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) >= 6:
                frame_id = int(parts[0])
                x = float(parts[2])
                y = float(parts[3])
                w = float(parts[4])
                h = float(parts[5])
                conf = float(parts[6]) if len(parts) > 6 else 1.0
                
                detections[frame_id].append({
                    'x': x, 'y': y, 'w': w, 'h': h, 'conf': conf
                })
    
    return detections


def iou(box1, box2):
    """
    è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IOU (Intersection over Union)
    box format: {x, y, w, h}
    """
    # è½¬æ¢ä¸º (x1, y1, x2, y2) æ ¼å¼
    x1_min, y1_min = box1['x'], box1['y']
    x1_max, y1_max = x1_min + box1['w'], y1_min + box1['h']
    
    x2_min, y2_min = box2['x'], box2['y']
    x2_max, y2_max = x2_min + box2['w'], y2_min + box2['h']
    
    # è®¡ç®—äº¤é›†
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)
    
    if inter_x_max < inter_x_min or inter_y_max < inter_y_min:
        return 0.0
    
    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)
    
    # è®¡ç®—å¹¶é›†
    box1_area = box1['w'] * box1['h']
    box2_area = box2['w'] * box2['h']
    union_area = box1_area + box2_area - inter_area
    
    return inter_area / union_area if union_area > 0 else 0.0


def evaluate_detections(pred_file, gt_file, iou_threshold=0.5):
    """
    è¯„ä¼°æ£€æµ‹ç»“æœ
    """
    print("\n" + "="*60)
    print("ğŸ“Š DETECTION EVALUATION")
    print("="*60)
    
    # è§£ææ–‡ä»¶
    predictions = parse_detections(pred_file)
    ground_truth = parse_detections(gt_file)
    
    print(f"\nğŸ“ Prediction file: {pred_file}")
    print(f"   Total frames: {len(predictions)}")
    total_pred = sum(len(boxes) for boxes in predictions.values())
    print(f"   Total detections: {total_pred}")
    
    print(f"\nğŸ“ Ground truth file: {gt_file}")
    print(f"   Total frames: {len(ground_truth)}")
    total_gt = sum(len(boxes) for boxes in ground_truth.values())
    print(f"   Total ground truth boxes: {total_gt}")
    
    # è·å–æ‰€æœ‰å¸§
    all_frames = set(predictions.keys()) | set(ground_truth.keys())
    print(f"\nğŸ“ˆ Total frames to evaluate: {len(all_frames)}")
    
    # è®¡ç®—TP, FP, FN
    tp = 0
    fp = 0
    fn = 0
    
    matched_pred = defaultdict(set)  # è®°å½•å“ªäº›é¢„æµ‹è¢«åŒ¹é…
    matched_gt = defaultdict(set)    # è®°å½•å“ªäº›GTè¢«åŒ¹é…
    
    for frame_id in sorted(all_frames):
        preds = predictions.get(frame_id, [])
        gts = ground_truth.get(frame_id, [])
        
        # æ’åºï¼ˆæŒ‰ç½®ä¿¡åº¦é™åºï¼‰
        preds_sorted = sorted(preds, key=lambda x: x['conf'], reverse=True)
        
        # ä¸ºæ¯ä¸ªé¢„æµ‹æ‰¾æœ€å¥½çš„åŒ¹é…GT
        for pred_idx, pred in enumerate(preds_sorted):
            best_iou = 0
            best_gt_idx = -1
            
            for gt_idx, gt in enumerate(gts):
                if gt_idx in matched_gt[frame_id]:
                    continue  # è¿™ä¸ªGTå·²ç»è¢«åŒ¹é…è¿‡
                
                curr_iou = iou(pred, gt)
                if curr_iou > best_iou:
                    best_iou = curr_iou
                    best_gt_idx = gt_idx
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºTPæˆ–FP
            if best_iou >= iou_threshold and best_gt_idx >= 0:
                tp += 1
                matched_pred[frame_id].add(pred_idx)
                matched_gt[frame_id].add(best_gt_idx)
            else:
                fp += 1
        
        # æœªåŒ¹é…çš„GTä¸ºFN
        fn += len(gts) - len(matched_gt[frame_id])
    
    # è®¡ç®—æŒ‡æ ‡
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\n" + "-"*60)
    print(f"ğŸ“Š METRICS (IOU threshold: {iou_threshold})")
    print("-"*60)
    print(f"True Positives (TP):   {tp}")
    print(f"False Positives (FP):  {fp}")
    print(f"False Negatives (FN):  {fn}")
    print(f"\nPrecision: {precision:.4f} ({tp}/{tp+fp})")
    print(f"Recall:    {recall:.4f} ({tp}/{tp+fn})")
    print(f"F1-Score:  {f1:.4f}")
    
    # æ£€æµ‹ç‡
    detection_rate = tp / total_gt if total_gt > 0 else 0
    print(f"\nDetection Rate: {detection_rate:.2%} ({tp}/{total_gt})")
    
    # è¯¯æ£€ç‡
    false_alarm_rate = fp / total_pred if total_pred > 0 else 0
    print(f"False Alarm Rate: {false_alarm_rate:.2%} ({fp}/{total_pred})")
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp,
        'fp': fp,
        'fn': fn,
        'detection_rate': detection_rate
    }


def compare_frame_by_frame(pred_file, gt_file, sample_frames=10):
    """
    é€å¸§å¯¹æ¯”
    """
    print("\n" + "="*60)
    print("ğŸ” FRAME-BY-FRAME COMPARISON")
    print("="*60)
    
    predictions = parse_detections(pred_file)
    ground_truth = parse_detections(gt_file)
    
    all_frames = sorted(set(predictions.keys()) | set(ground_truth.keys()))
    
    # éšæœºé€‰æ‹©æ ·æœ¬å¸§
    if len(all_frames) > sample_frames:
        sample_indices = np.random.choice(len(all_frames), sample_frames, replace=False)
        sample_frames_list = [all_frames[i] for i in sorted(sample_indices)]
    else:
        sample_frames_list = all_frames[:sample_frames]
    
    print(f"\næ˜¾ç¤º {len(sample_frames_list)} ä¸ªæ ·æœ¬å¸§çš„å¯¹æ¯”:\n")
    
    for frame_id in sample_frames_list:
        preds = predictions.get(frame_id, [])
        gts = ground_truth.get(frame_id, [])
        
        print(f"Frame {frame_id:06d}:")
        print(f"  Predictions: {len(preds):3d} boxes")
        print(f"  Ground Truth: {len(gts):3d} boxes")
        
        if len(preds) == 0 and len(gts) > 0:
            print(f"  âš ï¸  MISS! Expected {len(gts)} detections")
        elif len(preds) > 0 and len(gts) == 0:
            print(f"  âš ï¸  FALSE ALARM! Detected {len(preds)} boxes but GT is empty")
        elif len(preds) == len(gts):
            print(f"  âœ… Count matches")
        else:
            print(f"  âš ï¸  Count mismatch: {len(preds)} vs {len(gts)}")
    
    return sample_frames_list


if __name__ == '__main__':
    # æ–‡ä»¶è·¯å¾„
    pred_file = r'D:\UAV\YOLOv12-BoT-SORT-ReID\test_results\UAVSwarm-44\detections.txt'
    gt_file = r'D:\UAV\YOLOv12-BoT-SORT-ReID\data\UAVSwarm-dataset-master\test\UAVSwarm-44\det\det.txt'
    
    print("\nğŸš€ Detection Evaluation Script")
    print(f"Comparing prediction vs ground truth")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(pred_file):
        print(f"âŒ Prediction file not found: {pred_file}")
        exit(1)
    
    if not os.path.exists(gt_file):
        print(f"âŒ Ground truth file not found: {gt_file}")
        exit(1)
    
    # è¯„ä¼°
    metrics = evaluate_detections(pred_file, gt_file, iou_threshold=0.5)
    
    # é€å¸§å¯¹æ¯”
    compare_frame_by_frame(pred_file, gt_file, sample_frames=15)
    
    print("\n" + "="*60)
    print("âœ… Evaluation Complete!")
    print("="*60)
